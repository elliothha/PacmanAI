{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065d5e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b3477",
   "metadata": {},
   "source": [
    "## Initialization & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f003161",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ALE/MsPacman-v5\", render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d41d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs = observation of the initial state, TYPE=numpy array\n",
    "# element of the observation space: Box(0, 255, (210, 160, 3), uint8)\n",
    "# values in obs space = [0, 255], uint8\n",
    "# each obs = (height in pixels, width in pixels, 3 RGB color channels)\n",
    "# i.e., each obs is a color image of the current game screen\n",
    "\n",
    "# info = contains auxiliary info about game state, TYPE=dict\n",
    "# {\n",
    "# 'lives': 3,\n",
    "# 'episode_frame_number': 0,\n",
    "# 'frame_number': 0,\n",
    "# }\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# action = env.action_space.sample()\n",
    "# obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# obs = next observed state. TYPE=numpy array\n",
    "# reward = reward from taking current action. TYPE=float\n",
    "# terminated = bool\n",
    "# truncated = bool\n",
    "# info = dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156d636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "obs, info = env.reset(seed=seed)\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "initial_epsilon = 1.0\n",
    "final_epsilon = 0.1\n",
    "epsilon_decay = 0.995\n",
    "update_frequency = 1000\n",
    "discount_factor = 0.95\n",
    "n_episodes = 10000\n",
    "buffer_size = 50000\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51415ecc",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd353d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_frame(frame):\n",
    "    plt.imshow(frame, cmap=matplotlib.cm.Greys_r)\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    \n",
    "def preprocess_frame(frame, new_width=84, new_height=84):\n",
    "    '''\n",
    "    Preprocesses the input frame for the RL model.\n",
    "    Resizes, grayscales, and normalizes the frame.\n",
    "    '''\n",
    "    image = Image.fromarray(frame)\n",
    "    image = image.resize((new_width, new_height))\n",
    "    image = image.convert('L')\n",
    "    processed_frame = np.array(image) / 255.0\n",
    "    \n",
    "    return processed_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b7bbc",
   "metadata": {},
   "source": [
    "## Convolutional Dueling Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3def494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDDQN(nn.Module):\n",
    "    def __init__(self, input_dim=4, output_dim=env.action_space.n, hidden_dim=512):\n",
    "        super(ConvDDQN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # input tensor (BS,4,84,84) -> conv1 (BS,32,20,20) -> conv2 (BS,64,9,9) -> conv3 (BS,64,7,7)\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_dim, out_channels=32, kernel_size=8, stride=4, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # input (BS, 64*7*7) -> linear1 (BS, 512) -> linear2 (BS, 1)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7, out_features=self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=1)\n",
    "        )\n",
    "        \n",
    "        # input (BS, 64*7*7) -> linear1 (BS, 512) -> linear2 (BS, output_dim)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7, out_features=self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.hidden_dim, out_features=self.output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.conv_layer(state)\n",
    "        # flatten (BS,64,7,7) -> (BS, *) = (BS,64*7*7)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        \n",
    "        # values = (BS,1), advantages = (BS, output_dim)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        \n",
    "        # qvals = (BS, output_dim)\n",
    "        qvals = values + (advantages - advantages.mean())\n",
    "        \n",
    "        return qvals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890793a0",
   "metadata": {},
   "source": [
    "## Replay Buffer Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df9ee56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    # experience = (state_t, action_t, reward_t, state_{t+1}, done)\n",
    "    # state_t = (1, 4, 84, 84) TENSOR of [frame_{t-3}, frame_{t-2}, frame_{t-1}, frame_t] stacked\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # Get the current size of the buffer\n",
    "        buffer_size = len(self.buffer)\n",
    "\n",
    "        # Ensure that the batch size does not exceed the buffer size\n",
    "        sample_size = min(batch_size, buffer_size)\n",
    "\n",
    "        # Randomly sample indices based on the current buffer size\n",
    "        idx = np.random.choice(\n",
    "            np.arange(buffer_size),\n",
    "            size = sample_size,\n",
    "            replace = False\n",
    "        )\n",
    "        \n",
    "        return [self.buffer[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35edf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_buffer(\n",
    "    replay_buffer,\n",
    "    primary_network, \n",
    "    initial_epsilon, \n",
    "    final_epsilon,\n",
    "    n_episodes, \n",
    "    stack_len=4\n",
    "):   \n",
    "    epsilon_decay = (initial_epsilon - final_epsilon) / n_episodes\n",
    "    epsilon = initial_epsilon\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        frame, info = env.reset()\n",
    "        frame = preprocess_frame(frame)\n",
    "        done = False\n",
    "        \n",
    "        # Reset the stacked frames at the beginning of each episode\n",
    "        stacked_frames = deque([frame for _ in range(stack_len)], maxlen=stack_len)          \n",
    "        state = np.stack(stacked_frames, axis=0)  # (4, 84, 84)\n",
    "\n",
    "        while not done:\n",
    "            # turns this single np state (4,84,84) into (1,4,84,84) tensor for batch dim\n",
    "            if state.shape != (1,4,84,84):\n",
    "                state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "                \n",
    "            if np.random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    # get qvals and best action\n",
    "                    qvals = primary_network(state)\n",
    "                    action = torch.argmax(qvals).item()\n",
    "                    \n",
    "            next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            next_frame = preprocess_frame(next_frame)\n",
    "            stacked_frames.append(next_frame)\n",
    "            next_state = np.stack(stacked_frames, axis=0)\n",
    "            next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device) #(1, 4, 84, 84)\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            \n",
    "            experience = (state, action, reward, next_state, done)\n",
    "            replay_buffer.add(experience)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        epsilon = max(final_epsilon, epsilon - epsilon_decay)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b8e09",
   "metadata": {},
   "source": [
    "## RL Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b38e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MsPacmanAgent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        device,\n",
    "        optimizer,\n",
    "        primary_network, \n",
    "        target_network, \n",
    "        replay_buffer, \n",
    "        learning_rate,\n",
    "        batch_size,\n",
    "        initial_epsilon,\n",
    "        final_epsilon,\n",
    "        epsilon_decay,\n",
    "        discount_factor,\n",
    "    ):\n",
    "        '''\n",
    "        primary_network:\n",
    "        target_network:\n",
    "        replay_buffer: A deque that stores experiences (S_t, A_t, R_t, S_{t+1}, done)\n",
    "            A state, S_t, is a preprocessed (84, 84, 4) stack of 4 observable frames\n",
    "        learning_rate: For training the DDQN weights, high LR = more significant weight updates\n",
    "        initial_epsilon: Starting float value of epsilon for training\n",
    "        final_epsilon: Minimum float value epsilon is allowed to decay to over time, greater than 0\n",
    "        epsilon_decay: Determines how quickly epsilon will LINEARLY decay from initial to final vals\n",
    "        discount_factor (gamma): Influences bias towards immediate vs future rewards\n",
    "            High gamma = agent values future rewards more, low gamma = agent values immediate rewards\n",
    "        '''\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.primary_network = primary_network\n",
    "        self.target_network = target_network\n",
    "        self.replay_buffer = replay_buffer\n",
    "        \n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.epsilon = initial_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        self.discount_factor = discount_factor # \"gamma\" in the Bellman equation\n",
    "\n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        Implementation of the epsilon-greedy strategy for exploration-exploitation of the env\n",
    "        Args:\n",
    "            state: The current state of the game, (84, 84, 4) NumPy array\n",
    "            epsilon: Represents the probability of choosing a RANDOM action at each step\n",
    "                High epsilon = more likely to choose random (exploration)\n",
    "                Low epsilon = more likely to choose best-known action from q-vals (exploitation)\n",
    "                Ranges from [0, 1]\n",
    "        Return:\n",
    "            The action to take given the current state according to exploration or exploitation\n",
    "        '''\n",
    "        # with probability (epsilon), return a random action to explore the environment (exploration)\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "        \n",
    "        # with probability (1 - epsilon), act greedily, choose the best action to take (exploitation)\n",
    "        # obtain the best action to take from the max of q-vals returned from primary network\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # turns this single np state (4,84,84) into (1,4,84,84) tensor for batch dim\n",
    "                if state.shape != (1,4,84,84):\n",
    "                    state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "                # get qvals and best action\n",
    "                qvals = self.primary_network(state)\n",
    "                best_action = torch.argmax(qvals).item()\n",
    "            return best_action\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        Bellman Eq: Q(s,a) = r + gamma * argmax_{a'}Q(s',a')\n",
    "        \n",
    "        Q(s,a) = Predicted Q-Value for the action, a, actually taken in state, s\n",
    "        represents the DDQN's current estimate of the total expected future reward\n",
    "        (immediate reward + future rewards) for taking action, a, in state, s\n",
    "\n",
    "        Target Q-Value = Immediate reward, r, plus the discounted max Q-Val for next state, s'\n",
    "        represents what the Q-Val should ideally be\n",
    "        gamma = Discount factor hyperparameter\n",
    "        argmax_{a'}Q(s',a') = best future reward the agent expects to achieve from next state onward\n",
    "        '''\n",
    "        # If the replay buffer doesn't have enough experiences, we return and wait for population\n",
    "        if len(self.replay_buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        experiences = self.replay_buffer.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = torch.cat(states, dim=0).to(self.device)\n",
    "        actions = (torch.stack([torch.tensor(a, dtype=torch.long) for a in actions])).to(self.device)\n",
    "        rewards = (torch.stack([torch.tensor(r, dtype=torch.float32) for r in rewards])).to(self.device)\n",
    "        next_states = torch.cat(next_states, dim=0).to(self.device)\n",
    "        dones = (torch.stack([torch.tensor(d, dtype=torch.float32) for d in dones])).to(self.device)\n",
    "        # shape = (batch_size, 4, 84, 84)\n",
    "        \n",
    "        # predicted qvals = Q(s,a)\n",
    "        # self.primary_network(states) = (batch_size, env.action_space.n), \n",
    "        # qvals for all actions taken in each state in batch\n",
    "        # actions = (batch_size,), indices of the action taken for each state in the batch\n",
    "        # actions.unsqueeze(-1) = (batch_size, 1)\n",
    "        # qvals.gather(1, actions) = picks out the qval for the specific action in each state\n",
    "        # shape is (batch_size, 1) still\n",
    "        # .squeeze(-1) turns this into (batch_size,)\n",
    "        # (!!!) represents the primary DDQN's pred. for the qval for the action taken in each state\n",
    "        predicted_qvals = self.primary_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # target qvals = r + gamma * argmax_{a'}Q(s',a')\n",
    "        # self.target_network(next_states) = (batch_size, env.action_space.n), \n",
    "        # qvals for all actions taken in each next_state in batch\n",
    "        # next_qvals.max(dim=1)[0] = (batch_size,)\n",
    "        # the max qval across the action dim (dim=1) for each state\n",
    "        # target_qvals = (batch_size,) + gamma * (batch_size,) * (1 - (batch_size,)) = (batch_size,)\n",
    "        # (!!!) represents what the Q-Val should ideally be under the Bellman Eqn.\n",
    "        next_state_qvals = self.target_network(next_states).max(dim=1)[0]\n",
    "        target_qvals = rewards + (self.discount_factor * next_state_qvals * (1 - dones))\n",
    "        \n",
    "        loss = F.mse_loss(input=predicted_qvals, target=target_qvals)\n",
    "        \n",
    "        # backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # returns a scalar value for the MSE loss\n",
    "        return loss.item()\n",
    "        \n",
    "    def evaluate(self, n_eval_episodes=100):\n",
    "        self.primary_network.eval()\n",
    "        eval_env = gym.make(\"ALE/MsPacman-v5\", render_mode=None)\n",
    "        \n",
    "        total_rewards = []\n",
    "        \n",
    "        for episode in range(n_eval_episodes):\n",
    "            frame, info = eval_env.reset()\n",
    "            frame = preprocess_frame(frame)\n",
    "            done = False\n",
    "                        \n",
    "            # Reset the stacked frames at the beginning of each episode\n",
    "            stacked_frames = deque([frame for _ in range(4)], maxlen=4)          \n",
    "            state = np.stack(stacked_frames, axis=0)  # (4, 84, 84)\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            \n",
    "            episode_reward = 0\n",
    "            \n",
    "            while not done:\n",
    "                action = agent.select_action(state)\n",
    "        \n",
    "                next_frame, reward, terminated, truncated, info = eval_env.step(action)\n",
    "\n",
    "                next_frame = preprocess_frame(next_frame)\n",
    "                stacked_frames.append(next_frame)\n",
    "                next_state = np.stack(stacked_frames, axis=0)\n",
    "                next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n",
    "\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "        \n",
    "        average_reward = sum(total_rewards) / n_eval_episodes\n",
    "        print(f\"Average Reward Over {n_eval_episodes} Episodes: {average_reward}\\n\")\n",
    "        \n",
    "        eval_env.close()\n",
    "        self.primary_network.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646c975a",
   "metadata": {},
   "source": [
    "## Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6704fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(max_size=buffer_size)\n",
    "\n",
    "primary_network = ConvDDQN().to(device)\n",
    "target_network = ConvDDQN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(primary_network.parameters(), lr=lr)\n",
    "\n",
    "agent = MsPacmanAgent(\n",
    "    device=device,\n",
    "    optimizer=optimizer,\n",
    "    primary_network=primary_network,\n",
    "    target_network=target_network,\n",
    "    replay_buffer=replay_buffer,\n",
    "    learning_rate=lr,\n",
    "    batch_size=batch_size,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    discount_factor=discount_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca4ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████            | 16688/20000 [4:01:30<1:02:29,  1.13s/it]"
     ]
    }
   ],
   "source": [
    "populate_buffer(\n",
    "    replay_buffer=replay_buffer,\n",
    "    primary_network=primary_network,\n",
    "    initial_epsilon=initial_epsilon,\n",
    "    final_epsilon=final_epsilon,\n",
    "    n_episodes=20000,\n",
    "    stack_len=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c59e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    frame, info = env.reset()\n",
    "    frame = preprocess_frame(frame)\n",
    "    done = False\n",
    "    \n",
    "    # Reset the stacked frames at the beginning of each episode\n",
    "    stacked_frames = deque([frame for _ in range(4)], maxlen=4)          \n",
    "    state = np.stack(stacked_frames, axis=0)  # (4, 84, 84)\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device) # (1,4,84,84) tensor\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        \n",
    "        next_frame, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        next_frame = preprocess_frame(next_frame)\n",
    "        stacked_frames.append(next_frame)\n",
    "        next_state = np.stack(stacked_frames, axis=0) # (4, 84, 84)\n",
    "        next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device) # (1,4,84,84) tensor\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        agent.replay_buffer.add(experience)\n",
    "        \n",
    "        loss = agent.train()\n",
    "        if loss is not None:\n",
    "            training_loss.append(loss)\n",
    "    \n",
    "        state = next_state\n",
    "        \n",
    "    if episode != 0 and episode % update_frequency == 0:\n",
    "        agent.target_network.load_state_dict(agent.primary_network.state_dict())\n",
    "            \n",
    "        average_loss = sum(training_loss[-update_frequency:]) / update_frequency\n",
    "        print(f\"Episode {episode}, Average Loss: {average_loss}\")\n",
    "            \n",
    "        agent.evaluate(n_eval_episodes=100)\n",
    "        \n",
    "    \n",
    "    agent.decay_epsilon()\n",
    "\n",
    "plt.plot(training_loss)\n",
    "plt.title(\"Training Loss Over Episodes\")\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6c7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
